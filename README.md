[![Crawl Data](https://github.com/mrtrkmn/orbi/actions/workflows/execute.yaml/badge.svg)](https://github.com/mrtrkmn/orbi/actions/workflows/execute.yaml)

This is a simple crawler that crawls data from two websites currently:

- [https://www.ipo.gov.uk](https://www.ipo.gov.uk)
- [https://www.sec.gov/edgar/searchedgar/companysearch](https://www.sec.gov/edgar/searchedgar/companysearch)
- [https://www.bvdinfo.com/de-de/unsere-losungen/daten/international/orbis](https://www.bvdinfo.com/de-de/unsere-losungen/daten/international/orbis)

for company and patent related data.


## How to run 

There are two seperate files which are crawl_data.py and orbi.py. The main file is orbi.py which executes batch search on Orbis database based on
the csv file generated by crawl_data.py. The function on crawl_data.py is imported to orbi.py to generate csv file to feed the batch search on Orbis. 

### On Local Dev Machine

Since on Github actions, the script is using environment variables, it is required to have the environment variables set on your local machine.
Providing all environment variables through commandline would be a bit tedious, so I have created a config file which is used by the script to load the environment variables. Check sample config file from [here](./config/config-sample.yaml).

- Setup the requirements 

```bash 
$ python3 -m venv venv
$ source venv/bin/activate
$ pip install -r requirements.txt
```
- After setting up virtual environment, and installing requirements you can run the script by running the following command:

```bash
$ LOCAL_DEV=True CONFIG_PATH=./config/config.yaml python orbi.py
```

Make sure that you are defining the path to the config file correctly.


### On Remote

- The action can be triggered through [actions tab](https://github.com/mrtrkmn/orbi/actions/workflows/execute.yaml) on Github. Right side of the page, you can see the 'Run workflow' button to trigger the action.


- The action is also integrated with Slack. On the Slack channel, the same action can be triggered by typing `/run-orbis-crawler`. The notifications will be sent to the Slack channel #idp-data-c


To run the crawler seperately , you can run the following command:

```bash
$ python crawl_data.py
```
Check the main function in crawler.py for more details.


- `crawl_data.py` can generate csv file with; name, city, country and CIK number of the companies. Name, city and country are scraped from sec.gov website.(CIK number of the companies provided by a xlsx file)
- `crawl_data.py` can also find publications of the companies from ipo.gov.uk website. 


Automation of Orbis database access and batch search on Orbis database

- `orbi.py` can access Orbis database, execute batch search by providing the csv file generated by `crawl_data.py`, add/remove columns to enchance the search results and export the results to csv file.

- Currently orbi.py file will produce following files, in order to download them you can use the link send to Slack, and append with following file names below.


```raw
orbis_aggregated_data_{timestamp}.csv : example --> orbis_aggregated_data_13_01_2023.csv
orbis_aggregated_data_{timestamp}.xlsx : example --> orbis_aggregated_data_13_01_2023.xlsx
orbis_aggregated_data_licensee_{timestamp}.xlsx : example --> orbis_aggregated_data_licensee_14_01_2023.xlsx
orbis_aggregated_data_licensor_{timestamp}.xlsx : example --> orbis_aggregated_data_licensor_14_01_2023.xlsx
orbis_data_licensee_{timestamp}.csv : example --> orbis_data_licensee_14_01_2023.csv
orbis_data_licensee_14_01_2023.xlsx : example --> orbis_data_licensee_14_01_2023.xlsx
orbis_data_licensee_guo_{timestamp}.csv : example --> orbis_data_licensee_guo_14_01_2023.csv
orbis_data_licensee_guo_{timestamp}.xlsx : example --> orbis_data_licensee_guo_14_01_2023.xlsx
orbis_data_licensee_ish_{timestamp}.csv : example --> orbis_data_licensee_ish_14_01_2023.csv
orbis_data_licensee_ish_{timestamp}.xlsx : example --> orbis_data_licensee_ish_14_01_2023.xlsx
orbis_data_licensor_{timestamp}.csv  : example --> orbis_data_licensor_14_01_2023.csv
orbis_data_licensor_{timestamp}.xlsx : example --> orbis_data_licensor_14_01_2023.xlsx
orbis_data_licensor_guo_{timestamp}.csv : example --> orbis_data_licensor_guo_14_01_2023.csv
orbis_data_licensor_guo_{timestamp}.xlsx : example --> orbis_data_licensor_guo_14_01_2023.xlsx
orbis_data_licensor_ish_{timestamp}.csv : example --> orbis_data_licensor_ish_14_01_2023.csv
orbis_data_licensor_ish_{timestamp}.xlsx : example --> orbis_data_licensor_ish_14_01_2023.xlsx
- sample_data.xlsx
```
Data is accessible through: **link + file name** 

- The CSV files are generated through the `crawl_data.py` file.



```raw
orbis_aggregated_data_{timestamp}.csv 
orbis_data_licensee_{timestamp}.csv
orbis_data_licensee_guo_{timestamp}.csv
orbis_data_licensee_ish_{timestamp}.csv
orbis_data_licensor_{timestamp}.csv
orbis_data_licensor_guo_{timestamp}.csv
orbis_data_licensor_ish_{timestamp}.csv
```


- The XLSX files are generated through the orbi.py` by conducting batch search on Orbis database. 


```raw
orbis_aggregated_data_{timestamp}.xlsx
orbis_aggregated_data_licensee_{timestamp}.xlsx
orbis_aggregated_data_licensor_{timestamp}.xlsx
orbis_data_licensee_{timestamp}.xlsx
orbis_data_licensee_guo_{timestamp}.xlsx
orbis_data_licensee_ish_{timestamp}.xlsx
orbis_data_licensor_{timestamp}.xlsx
orbis_data_licensor_guo_{timestamp}.xlsx
orbis_data_licensor_ish_{timestamp}.xlsx

```


## Slack Integration 

Currently, action results are uploaded to AWS S3 service and accesible with the link sent to private Slack channel. 
The files can be downloaded as decribed in the slack channel. 



## Main Workflow 

Beside the given main workflow given below, there are other options which can be used with this repository. 

The workflow is subject to change in time. 

<img src="https://user-images.githubusercontent.com/13614433/213180207-3855244f-29f9-42c5-ab7f-9655f0c78479.png"  width="750" height="300">






