[![Crawl Data](https://github.com/mrtrkmn/orbi/actions/workflows/execute.yaml/badge.svg)](https://github.com/mrtrkmn/orbi/actions/workflows/execute.yaml)

This is a simple crawler that crawls data from two websites currently:

- [https://www.ipo.gov.uk](https://www.ipo.gov.uk)
- [https://www.sec.gov/edgar/searchedgar/companysearch](https://www.sec.gov/edgar/searchedgar/companysearch)
- [https://www.bvdinfo.com/de-de/unsere-losungen/daten/international/orbis](https://www.bvdinfo.com/de-de/unsere-losungen/daten/international/orbis)

for company and patent related data.

- _**main branch:**_ [possible] sync with `run-on-github` branch. 
- _**run-on-self-hosted:**_ runs on self-hosted computer - updated frequently than main branch. Create PR to main to receive updates. 


## How to run 

[./orbi](https://github.com/mrtrkmn/orbi/tree/main/orbi) contains the main script which is used to run the crawler. It contains two different classes to crawl data from the websites.

- Class Crawler: This class is used to crawl data from [ipo.gov.uk](https://www.ipo.gov.uk) and sec.gov website. It is used to generate csv file with; name, city, country and CIK number of the companies. Name, city and country are scraped from sec.gov website.(CIK number of the companies provided by a xlsx file)

- Class Orbis: This class is responsible for crawling data from [Orbis database](https://www.bvdinfo.com/de-de/unsere-losungen/daten/international/orbis). It is using batch search to find the companies from the csv file generated by Crawler class. It also adds/removes columns to enchance the search results and export the results to xlsx file. 

All process is automated by using selenium and chromedriver.

### On Local Dev Machine

Since on Github actions, the script is using environment variables, it is required to have the environment variables set on your local machine.
Providing all environment variables through commandline would be a bit tedious, so I have created a config file which is used by the script to load the environment variables. Check sample config file from [here](./config/config-sample.yaml).

- Setup the requirements 

```bash 
$ python3 -m venv venv
$ source venv/bin/activate
$ pip install -r requirements.txt
```
- After setting up virtual environment, and installing requirements you can run the script by running the following command:

```bash
$ LOCAL_DEV=True CONFIG_PATH=./config/config.yaml python orbi/orbi.py
```

Make sure that you are defining the path to the config file correctly.


### On Remote

- The action can be triggered through [actions tab](https://github.com/mrtrkmn/orbi/actions/workflows/execute.yaml) on Github. Right side of the page, you can see the 'Run workflow' button to trigger the action.

To run the crawler classs seperately , check out the commented code in [./orbi/crawl.py](https://github.com/mrtrkmn/orbi/tree/main/orbi/orbi/crawl.py)` file.

Specifically, this line: [./orbi/orbi.py#494](https://github.com/mrtrkmn/orbi/tree/main/orbi/orbi/orbi.py#L494)


Automation of Orbis database access and batch search on Orbis database

- `orbi.py` can access Orbis database, execute batch search by providing the csv file generated by Crawler class, add/remove columns to enchance the search results and export the results to csv file.

- Currently orbi.py file will produce following files, in order to download them you can use the link send to Slack, and append with following file names below.

<details>
<summary>Produced files by Orbi class</summary>
<pre>
orbis_aggregated_data_{timestamp}.csv : example --> orbis_aggregated_data_13_01_2023.csv
orbis_aggregated_data_{timestamp}.xlsx : example --> orbis_aggregated_data_13_01_2023.xlsx
orbis_aggregated_data_licensee_{timestamp}.xlsx : example --> orbis_aggregated_data_licensee_14_01_2023.xlsx
orbis_aggregated_data_licensor_{timestamp}.xlsx : example --> orbis_aggregated_data_licensor_14_01_2023.xlsx
orbis_data_licensee_{timestamp}.csv : example --> orbis_data_licensee_14_01_2023.csv
orbis_data_licensee_14_01_2023.xlsx : example --> orbis_data_licensee_14_01_2023.xlsx
orbis_data_licensee_guo_{timestamp}.csv : example --> orbis_data_licensee_guo_14_01_2023.csv
orbis_data_licensee_guo_{timestamp}.xlsx : example --> orbis_data_licensee_guo_14_01_2023.xlsx
orbis_data_licensee_ish_{timestamp}.csv : example --> orbis_data_licensee_ish_14_01_2023.csv
orbis_data_licensee_ish_{timestamp}.xlsx : example --> orbis_data_licensee_ish_14_01_2023.xlsx
orbis_data_licensor_{timestamp}.csv  : example --> orbis_data_licensor_14_01_2023.csv
orbis_data_licensor_{timestamp}.xlsx : example --> orbis_data_licensor_14_01_2023.xlsx
orbis_data_licensor_guo_{timestamp}.csv : example --> orbis_data_licensor_guo_14_01_2023.csv
orbis_data_licensor_guo_{timestamp}.xlsx : example --> orbis_data_licensor_guo_14_01_2023.xlsx
orbis_data_licensor_ish_{timestamp}.csv : example --> orbis_data_licensor_ish_14_01_2023.csv
orbis_data_licensor_ish_{timestamp}.xlsx : example --> orbis_data_licensor_ish_14_01_2023.xlsx
- sample_data.xlsx
</pre>
</details>

- Data is accessible through: **link + file name** 

<details>
<summary>Produced files by Crawler class</summary>
<pre>
orbis_aggregated_data_{timestamp}.csv 
orbis_data_licensee_{timestamp}.csv
orbis_data_licensee_guo_{timestamp}.csv
orbis_data_licensee_ish_{timestamp}.csv
orbis_data_licensor_{timestamp}.csv
orbis_data_licensor_guo_{timestamp}.csv
orbis_data_licensor_ish_{timestamp}.csv
</pre>
</details>


- The XLSX files are generated through the [./orbi/orbi.py](https://github.com/mrtrkmn/orbi/tree/main/orbi/orbi/orbi.py) by conducting batch search on Orbis database.



<details>
<summary>Produced XLSX files  by Orbi class - END RESULT - </summary>
<pre>
orbis_aggregated_data_{timestamp}.xlsx
orbis_aggregated_data_licensee_{timestamp}.xlsx
orbis_aggregated_data_licensor_{timestamp}.xlsx
orbis_data_licensee_{timestamp}.xlsx
orbis_data_licensee_guo_{timestamp}.xlsx
orbis_data_licensee_ish_{timestamp}.xlsx
orbis_data_licensor_{timestamp}.xlsx
orbis_data_licensor_guo_{timestamp}.xlsx
orbis_data_licensor_ish_{timestamp}.xlsx
</pre>
</details>


## Slack Integration 

Currently, action results are uploaded to AWS S3 service and accesible with the link sent to private Slack channel. 
The files can be downloaded as decribed in the slack channel. 

### Run orbi from Slack

Orbi can be triggered on Github from slack when you are in tum-tim.slack.com workspace. 

Any user who writes in the message field of Slack the following command and press 'Enter', Orbi will start the process on Github: 

```bash 
/run-orbis-crawler 
```

You will receive a result as shown below from Slack. 

---

![how-to-run-orbi-from-slack](https://user-images.githubusercontent.com/13614433/222140712-86be7358-b90d-44fa-8cb8-6633a72d51e4.png)

---

After it is initialized, you will receive a message to #idp-data-c channel on Slack similar to the following:

---
![Initial Notification](https://user-images.githubusercontent.com/13614433/222143717-16cb6cfa-f9cb-4a10-a7aa-c71083e16ea4.png)

--- 
When it is done successfully, you will have a new notification with the link which provides access to data that similar to following: 

![Screenshot 2023-03-01 at 13 47 46](https://user-images.githubusercontent.com/13614433/222143625-3115dcc3-e49d-40ac-bc8d-2af362c66feb.png)

--- 

In case of error on the process, similar notification will be received as provided below: 


![Error notification](https://user-images.githubusercontent.com/13614433/222143963-d9d3443a-4e07-4adb-bcdd-a96360380934.png)


## Main Workflow 

Beside the given main workflow given below, there are other options which can be used with this repository. 

The workflow is subject to change in time. 
--- 
<img src="https://user-images.githubusercontent.com/13614433/213180207-3855244f-29f9-42c5-ab7f-9655f0c78479.png"  width="750" height="300">

## Batch Search Flow Chart 

The following flow chart shows the process of batch search done by Orbi.

<details>
<summary>Flowchart of the batch search functionality of Orbi.</summary>
<pre>
<img src="https://user-images.githubusercontent.com/13614433/227824562-0610cf9f-cd36-45df-b999-966e7d0a5cdd.png">
</pre>
</details>








