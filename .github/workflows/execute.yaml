
name: Crawl Data

on:
  # push:
  #   branches: [ "main" ]
  # pull_request:
  #   branches: [ "main" ]
  workflow_dispatch:
    inputs:
      message:
        description: 'Commit message'
        required: false
        default: 'Crawl data'
      data: 
        description: '
          The data link which contains list of licensee/licensor info. (e.g sample_data.xlsx)
          The link should be a public link which can be downloaded by wget command.          
          **** ONLY XLSX FILE IS SUPPORTED ! ****
          '
        required: false
        default: sample_data.xlsx

      send_data_on_completion:
        description: 'To receive data on search completion, check this option'
        type: boolean
        required: false
        default: false

      activate_parallel_exec:
        description: 'Enable to run the program in parallel. [ NOT ADVICED DURING WORKING HOURS - SINCE # OF MAX SIMULTANEOUS USER ACCESS IS 5 ]'
        type: boolean
        required: false
        default: false

      notify:
        description: 'To receive notifications and all search result files in one zip file on Slack channel, check this option'
        type: boolean
        required: false
        default: false

      check_on_sec:
        description: 'SEC.GOV website will be checked for additional data to aggregate. Check this option to enable it.'
        type: boolean
        required: false
        default: false

  repository_dispatch:
    types: crawl-data

env:
  # orbis access specific
  ORBIS_EMAIL_ADDRESS: ${{ secrets.ORBIS_EMAIL_ADDRESS }}
  ORBIS_PASSWORD: ${{ secrets.ORBIS_PASSWORD }}
  ORBIS_ACCESS_URL: ${{ secrets.ORBIS_ACCESS_URL }}
  ORBIS_BATCH_SEARCH_URL: ${{ secrets.ORBIS_BATCH_SEARCH_URL }}
  ORBIS_LOGOUT_URL: ${{ secrets.ORBIS_LOGOUT_URL }}

  # crawler specific

  DATA_DIR: /home/thinkpad/Desktop/github-self-hosted/actions-runner/_study/orbi/orbi/data/
  LOG_DIR: /home/thinkpad/Desktop/github-self-hosted/actions-runner/_study/orbi/orbi/logs/
  LOCAL_DEV: False
  DEFAULT_DATA_SOURCE: sample_data.xlsx
  MAX_RUN_ATTEMPTS: 3  # if it is unable to successfull in this number tries, the crawler will exit with failure
  RUN_TIMEOUT_IN_MN: 240 # If it is unable to fetch all data in 4 hours, the crawler will face with timeout
  
  
  #  DATA_DIR: /home/runner/work/orbi/orbi/data/
  #  LOG_DIR: /home/runner/work/orbi/orbi/logs/    
  
  # # aws specific
  # AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }} 
  # AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} 
  # AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} 
  # SOURCE_DIR: /home/runner/work/orbi/orbi/

  # slack specific
  SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL }}
  SLACK_DATA_CHANNEL: ${{ secrets.SLACK_DATA_CHANNEL }}
  SLACK_USERNAME: orbi
  SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
  SLACK_FOOTER: 'ðŸ¤“ automated bot Orbi'
  SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
  # artifact specific
  RETENTION_PERIOD: 20 # in days

  CHECK_ON_SEC: ${{ github.event.inputs.check_on_sec }}
permissions:
  contents: read

jobs:

  search_on_sec_gov:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: "pip" # cache pip dependencies

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

   # get timestamp in format DD-MM-YYYY
    - name: Get timestamp
      id: get-timestamp
      run: |
        echo "timestamp=$(date +'%d-%m-%Y')" >> $GITHUB_OUTPUT

    - name: (LICENSEE) Crawl data from SEC.GOV
      run: |
        python orbi/crawl.py --source_file ${{ env.DATA_DIR }}${{ env.DEFAULT_DATA_SOURCE }} --is_licensee True
      
    - name: (LICENSOR) Crawl data from SEC.GOV
      run: |
        python orbi/crawl.py --source_file ${{ env.DATA_DIR }}${{ env.DEFAULT_DATA_SOURCE }} --is_licensee False

    # upload csv and json files to azure blob storage
    
    # THIS PART NEEDS TO BE UPDATED WITH AN ACTIVE AZURE ACCOUNT CREDENTIALS 

    # python utils/upload_to_azure.py <file/dir> <container> <blob>
    # - name: Upload to Azure Blob
    #   run: |
    #     pip install azure-storage-blob
    #     python utils/upload_to_azure.py --dir_file_path /home/runner/work/orbi/orbi/data --container_name ${{ env.AZURE_CONTAINER_NAME }} --blob_name orbis-data-${{ steps.get-timestamp.outputs.timestamp }}
    #   env:
    #     ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
    #     ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
    #     RETENTION_DAYS: 30
    #     AZURE_CONTAINER_NAME: sec-gov-data
    #     SLACK_CHANNEL: ${{ secrets.SLACK_DATA_CHANNEL }}
    #   continue-on-error: true


# upload to artifacts 
    - name: Upload data to artifact 
      uses: actions/upload-artifact@v3
      with:
        name: sec-gov-data-${{ steps.get-timestamp.outputs.timestamp }}.zip
        path: |
           ${{ env.DATA_DIR }}
           !'${{ env.DATA_DIR }}sample_data.xlsx'
           !'${{ env.DATA_DIR }}sample_data_big.xlsx'
        retention-days: ${{ env.RETENTION_PERIOD }}
      continue-on-error: true


# -----------------------RUN ON ORBIS JOB-------------------------------------------
  search_on_orbis: # default working directory
    runs-on: self-hosted
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: "pip" # cache pip dependencies

    #**********************************************************************************************************************
    # PRECHECKS
    #**********************************************************************************************************************
    - name: Check input file name
      id: check_input_file_name
      # check if the input file name is referring to a file or url 
      run: |
        if [[ ${{ github.event.inputs.data }} == http* ]]; then
          echo "is_url=true" >> $GITHUB_OUTPUT
        else
          echo "is_url=false" >> $GITHUB_OUTPUT
        fi

    - name: Get file name
      if: ${{ github.event.inputs.data != '${{ env.DEFAULT_DATA_SOURCE }}' }}
      id: get-file-name
      run: |
        echo "file_name=$(basename ${{ github.event.inputs.data }})" >> $GITHUB_OUTPUT


    - name: Check file existence
      if: ${{ steps.check_input_file_name.outputs.is_url == 'false' }}
      run: |
        if [ ! -f ${{ env.DATA_DIR }}${{ steps.get-file-name.outputs.file_name }} ]; then
          echo "File ${{ steps.get-file-name.outputs.file_name }} does not exist in ${{ env.DATA_DIR }}. Exiting..."
          exit 1
        fi

    - name: Download data if provided
      if: ${{ steps.check_input_file_name.outputs.is_url == 'true' }}
      run: |
        wget ${{ github.event.inputs.data }}
        mv  ${{ steps.get-file-name.outputs.file_name }} ${{ env.DATA_DIR }} 

    - name: Check file existence
      if: ${{ steps.check_input_file_name.outputs.is_url == 'false' }}
      run: |
        if [ ! -f ${{ env.DATA_DIR }}${{ steps.get-file-name.outputs.file_name }} ]; then
          echo "File ${{ steps.get-file-name.outputs.file_name }} does not exist in ${{ env.DATA_DIR }}. Exiting..."
          exit 1
        fi

      # get timestamp in format DD-MM-YYYY
    - name: Get timestamp
      id: get-timestamp
      run: |
        echo "timestamp=$(date +'%d-%m-%Y')" >> $GITHUB_OUTPUT

    #**********************************************************************************************************************
    # INSTALL REQUIREMENTS
    #**********************************************************************************************************************

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Send Job Started Notification to Slack
      if: ${{ github.event.inputs.notify == 'true' }}
      uses: rtCamp/action-slack-notify@v2.2.0
      env:
        SLACK_COLOR: ${{ job.status }} # or a specific color like 'good' or '#ff00ff'
        SLACK_MESSAGE: ':bell: Orbi is started crawl data from Orbis database on  Github using the file ${{ github.event.inputs.data }}  as input data.'
        SLACK_TITLE: ':bell: Orbi is working on crawling data from Orbis database on Github'

    #**********************************************************************************************************************
    # RUN Orbi for batch search on Orbis
    #**********************************************************************************************************************
    - name: Execute the script
      uses: nick-fields/retry@v2
      id: batch_search
      with:
        timeout_minutes: ${{ env.RUN_TIMEOUT_IN_MN }} # 2 hours
        max_attempts: ${{ env.MAX_RUN_ATTEMPTS }} # 3 times retry if failed
        command: |
          python orbi/orbi.py
      env:
        DATA_SOURCE: ${{ steps.get-file-name.outputs.file_name }}
        SEND_DATA_ON_COMPLETION: ${{ github.event.inputs.send_data_on_completion }}
        CHECK_ON_SEC: ${{ github.event.inputs.check_on_sec }}
        PARALLEL_EXECUTION: ${{ github.event.inputs.activate_parallel_exec }}
    #**********************************************************************************************************************
    #  POST PROCESSING STEPS
    #**********************************************************************************************************************

    - name: Send Job Failed Notification to Slack
      if: ${{ failure() && steps.batch_search.conclusion == 'failure' && github.event.inputs.notify == 'true' }}
      uses: rtCamp/action-slack-notify@v2.2.0
      env:
        SLACK_COLOR: ${{ job.status }} # or a specific color like 'good' or '#ff00ff'
        SLACK_MESSAGE: ':x: Failed to run batch search, please check on Github. '
        SLACK_TITLE: ':x: Batch search failed at some point, in most cases this is caused due to slowness in the network (since it causes slow rendering of the page and leads to have no element we are looking for) !'


    # python utils/upload_to_azure.py <file/dir> <container> <blob>
    - name: Upload to Azure Blob
      run: |
        pip install azure-storage-blob
        python utils/upload_to_azure.py --dir_file_path ${{ env.DATA_DIR }} --container_name ${{ env.AZURE_CONTAINER_NAME }} --blob_name orbis-data-${{ steps.get-timestamp.outputs.timestamp }}
      env:
        ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
        ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
        RETENTION_DAYS: 7
        AZURE_CONTAINER_NAME: ${{ secrets.AZURE_CONTAINER_NAME }}
      continue-on-error: true

    # zip the data except sample_data.xlsx
    - name: Zip data
      run: |
        zip -j -r orbis-data-${{ steps.get-timestamp.outputs.timestamp }}.zip '${{ env.DATA_DIR }}' -x '${{ env.DATA_DIR }}sample_data.xlsx' -x '${{ env.DATA_DIR }}sample_data_big.xlsx' -x '${{ env.DATA_DIR }}${{ steps.get-file-name.outputs.file_name }}'

# do not upload for now
    # - name: Upload to S3 
    #   uses: jakejarvis/s3-sync-action@master
    #   with:
    #     args: --acl public-read --follow-symlinks --delete
    #   env:
    #     DEST_DIR: orbis-data-${{ steps.get-timestamp.outputs.timestamp }}

    - name: Send data to slack channel 
      if: ${{ success() && github.event.inputs.notify == 'true' }}
      run: |
        python utils/send_to_slack.py --file_path orbis-data-${{ steps.get-timestamp.outputs.timestamp }}.zip  --slack_channel ${{ env.SLACK_DATA_CHANNEL }} --message ' Prepared data from Orbis database search is done !. orbis-data-${{ steps.get-timestamp.outputs.timestamp }}.zip is attached.'
      continue-on-error: true


    # in case of using aws s3 bucket this message can be used 
    # curl -F chat_id=${{ secrets.TELEGRAM_CHAT_ID }} -F text="Data is ready ! Link: https://${{ secrets.AWS_S3_BUCKET }}.s3.${{ secrets.AWS_REGION }}.amazonaws.com/orbis-data-${{ steps.get-timestamp.outputs.timestamp }}" https://api.telegram.org/bot${{ secrets.TELEGRAM_BOT_TOKEN }}/sendMessage
    - name: Send data to Telegram 
      run: |
        curl -F chat_id=${{ secrets.TELEGRAM_CHAT_ID }} -F document=@orbis-data-${{ steps.get-timestamp.outputs.timestamp }}.zip https://api.telegram.org/bot${{ secrets.TELEGRAM_BOT_TOKEN }}/sendDocument
      continue-on-error: true


    - name: Upload data to artifact 
      uses: actions/upload-artifact@v3
      with:
        name: orbis-data-${{ steps.get-timestamp.outputs.timestamp }}.zip
        path: |
           ${{ env.DATA_DIR }}
           !'${{ env.DATA_DIR }}sample_data.xlsx'
           !'${{ env.DATA_DIR }}sample_data_big.xlsx'
        retention-days: ${{ env.RETENTION_PERIOD }}
      continue-on-error: true

    - name:  Send Job Success Notification to Slack
      if: ${{ success() && github.event.inputs.notify == 'true' }}
      uses: rtCamp/action-slack-notify@v2.2.0
      env:
        SLACK_COLOR: ${{ job.status }} # or a specific color like 'good' or '#ff00ff'
        SLACK_MESSAGE: ':rocket: Data is available to download from artifacts: link to artifacts: htttps://github.com/mrtrkmn/orbi/actions/runs/${{ github.run_id }}'
        # SLACK_MESSAGE: ':rocket: Operation is completed data link: https://${{ secrets.AWS_S3_BUCKET }}.s3.${{ secrets.AWS_REGION }}.amazonaws.com/orbis-data-${{ steps.get-timestamp.outputs.timestamp }}/ For more information check README page of the project: https://github.com/mrtrkmn/orbi'
        SLACK_TITLE: Data is ready to download append file name to the link !
        SLACK_USERNAME: orbi
        SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
        SLACK_FOOTER: 'ðŸ¤“ automated bot Orbi'
